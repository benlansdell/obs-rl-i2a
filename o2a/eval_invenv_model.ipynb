{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the inverse model\n",
    "\n",
    "Evaluate and visualize the performance of the environment model by seeing it visualize future states while a A2C agent plays the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't use the GPU for these runs:\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from env_model_minigrid import make_env, create_env_model, create_latentinverse_env_model\n",
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "from common.minigrid_util import num_pixels, mode_rewards, pix_to_target, rewards_to_target\n",
    "from a2c import get_actor_critic, CnnPolicy\n",
    "from i2a import convert_target_to_real\n",
    "import gym\n",
    "import gym_minigrid\n",
    "import common.minigrid_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create the environments we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"MiniGrid-Blocks-6x6-v0\"\n",
    "#env_name = \"MiniGrid-Blocks-8x8-v0\"\n",
    "#env_name = \"MiniGrid-Blocks-16x16-v0\"\n",
    "nenvs = 16\n",
    "nsteps = 5\n",
    "envs = [make_env(env_name) for i in range(nenvs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "ob_space = envs.observation_space.shape\n",
    "ac_space = envs.action_space\n",
    "num_actions = envs.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, go ahead and test the environment model in minigrid. This will use the A2C agent to play the game and the environment model to predict future states and rewards. This will visualize the imagined and real rewards and game states from the environment model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space (6, 6, 3)\n",
      "number of actions 5\n"
     ]
    }
   ],
   "source": [
    "#Make minigrid env...\n",
    "env = gym_minigrid.wrappers.ImgObsWrapper(gym.make(env_name))\n",
    "\n",
    "done = False\n",
    "states = env.reset()\n",
    "num_actions = ac_space.n\n",
    "nw, nh, nc = ob_space\n",
    "print('observation space', ob_space)\n",
    "print('number of actions', num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lansdell/projects/o2a/o2a/a2c.py:61: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /home/lansdell/projects/o2a/o2a/a2c.py:14: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/lansdell/projects/o2a/o2a/a2c.py:16: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "INFO:tensorflow:Restoring parameters from weights/a2c_400000.ckpt\n",
      "WARNING:tensorflow:From /home/lansdell/projects/o2a/o2a/env_model_minigrid.py:213: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "INFO:tensorflow:Restoring parameters from weights/env_model_inverse.ckpt\n",
      "Step: 0, Reward: -0.006944\n",
      "Step: 1, Reward: -0.006944\n",
      "Step: 2, Reward: -0.006944\n",
      "Step: 3, Reward: -0.006944\n",
      "Step: 4, Reward: -0.006944\n",
      "Step: 5, Reward: -0.006944\n",
      "Step: 6, Reward: -0.006944\n",
      "Step: 7, Reward: -0.006944\n",
      "Step: 8, Reward: -0.006944\n",
      "Step: 9, Reward: -0.006944\n",
      "Step: 10, Reward: -0.006944\n",
      "Step: 11, Reward: -0.006944\n",
      "Step: 12, Reward: -0.006944\n",
      "Step: 13, Reward: -0.006944\n",
      "Step: 14, Reward: -0.006944\n",
      "Step: 15, Reward: -0.006944\n",
      "Step: 16, Reward: 1.000000\n",
      "Step: 17, Reward: -0.006944\n",
      "Step: 18, Reward: -0.006944\n",
      "Step: 19, Reward: -0.006944\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "n_steps = 20\n",
    "pred_actions = np.zeros((n_steps, num_actions))\n",
    "act_states = np.zeros((n_steps, nw, nh, nc))\n",
    "act_rewards = np.zeros(n_steps)\n",
    "act_actions = np.zeros(n_steps)\n",
    "steps = 0\n",
    "with tf.Session() as sess:\n",
    "    # Load the actor\n",
    "    with tf.variable_scope('actor'):\n",
    "        actor_critic = get_actor_critic(sess, nenvs, nsteps, ob_space,\n",
    "                ac_space, CnnPolicy, should_summary=False)\n",
    "    #actor_critic.load('weights/a2c_100000.ckpt')\n",
    "    actor_critic.load('weights/a2c_400000.ckpt')\n",
    "\n",
    "    # Load the critic\n",
    "    with tf.variable_scope('env_model'):\n",
    "        env_model = create_latentinverse_env_model(ob_space, num_actions, num_pixels,\n",
    "                len(mode_rewards['regular']), should_summary=False)\n",
    "        #env_model = create_env_model(ob_space, num_actions, num_pixels,\n",
    "        #        len(mode_rewards['regular']), should_summary=False)\n",
    "\n",
    "    save_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='env_model')\n",
    "    loader = tf.train.Saver(var_list=save_vars)\n",
    "    loader.restore(sess, 'weights/env_model_inverse.ckpt')\n",
    "    \n",
    "    #while not done and steps < n_steps:\n",
    "    while steps < n_steps:\n",
    "        actions, _, _ = actor_critic.act(np.expand_dims(states, axis=0))\n",
    "        onehot_actions = np.zeros((1, num_actions))\n",
    "        onehot_actions[range(1), actions] = 1\n",
    "        next_states, reward, done, _ = env.step(actions[0])\n",
    "        ainv = sess.run([env_model.ainvprobs], \n",
    "                                       feed_dict={\n",
    "                env_model.input_states: np.expand_dims(states, axis=0),\n",
    "                env_model.target_states: np.expand_dims(next_states, axis = 0)\n",
    "            })\n",
    "        print(\"Step: %d, Reward: %f\"%(steps, reward))\n",
    "        pred_actions[steps,:] = ainv[0]\n",
    "        act_actions[steps] = actions\n",
    "        act_states[steps,:,:,:] = states\n",
    "        act_rewards[steps] = reward\n",
    "        states = next_states\n",
    "        steps += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 3., 0., 3., 0., 1., 1., 0., 4., 4., 4., 2., 1., 4., 2., 1., 0.,\n",
       "       4., 3., 2.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 0, 3, 0, 1, 1, 0, 0, 0, 0, 2, 1, 1, 2, 1, 0, 0, 0, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred_actions, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize...\n",
    "#If it goes blank then reached the done state\n",
    "for steps in range(n_steps):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(10,3))        \n",
    "    plt.title(\"Step %d. reward: %i\" % (steps,act_rewards[steps]))\n",
    "    plt.imshow(act_states[steps,:,:,:]*11./255)\n",
    "    plt.show()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seems better. Only one glitch in the model. The agent appears to overlap with the block on one frame, \n",
    "#or the agent just disappears. Thus:\n",
    "#Imagination doesn't contain the agent all the time....\n",
    "#X and Y dynamics now appear fixed... \n",
    "\n",
    "#There's still something funny with the dynamics when it interacts with the block. \n",
    "#...x and y must still be switched somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo:\n",
    "\n",
    "#Try out larger environments then what agent is trained on\n",
    "#Try out only learning latent representation\n",
    "# xx Test inverse model\n",
    "\n",
    "#Code framework for Obs RL:\n",
    "## Change I2A algorithm to:\n",
    "### a) take off-line state transitions -- generated from:\n",
    "###    i) A random agent\n",
    "###    ii) Many random agents\n",
    "###    iii) Ghost conditions\n",
    "### b) Replace actual action with a sample from the inverse model. Rest of algorithm is the same.\n",
    "\n",
    "## Implement test environment. Compare with:\n",
    "### Naive algorithm with no obs experience\n",
    "### Expert algorithm with 'on policy' experience from obs phase\n",
    "### RILO (if can find code...)\n",
    "\n",
    "#Meta learner instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
